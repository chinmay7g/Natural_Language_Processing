{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing package\n",
    "import spacy\n",
    "\n",
    "#loading pipeline\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieving data from Wiki\n",
    "\n",
    "import wikipedia\n",
    "\n",
    "def pages_to_sentences(*pages):\n",
    "    sentences = []\n",
    "    for page in pages:\n",
    "        p = wikipedia.page(page)\n",
    "        doc = nlp(p.content)\n",
    "        sentences += [sent.text for sent in doc.sents]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the data\n",
    "\n",
    "animal_sents = pages_to_sentences(\"Python(programming language)\")\n",
    "language_sents = pages_to_sentences('Reticulated python','Ball python')\n",
    "\n",
    "documents = animal_sents + language_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<833x2730 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8776 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training the bag of words model\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bag_of_words = CountVectorizer()\n",
    "bag_of_words.fit(documents)\n",
    "bag_of_words.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 219)\t1\n",
      "  (0, 1073)\t1\n",
      "  (0, 1170)\t1\n",
      "  (0, 1310)\t1\n",
      "  (0, 1327)\t1\n",
      "  (0, 1396)\t1\n",
      "  (0, 1431)\t1\n",
      "  (0, 1923)\t1\n",
      "  (0, 1944)\t1\n",
      "  (0, 1964)\t1\n",
      "  (1, 37)\t1\n",
      "  (1, 224)\t1\n",
      "  (1, 423)\t1\n",
      "  (1, 516)\t1\n",
      "  (1, 663)\t1\n",
      "  (1, 729)\t1\n",
      "  (1, 846)\t1\n",
      "  (1, 991)\t1\n",
      "  (1, 1126)\t1\n",
      "  (1, 1234)\t1\n",
      "  (1, 1344)\t1\n",
      "  (1, 1697)\t1\n",
      "  (1, 1722)\t1\n",
      "  (1, 1833)\t1\n",
      "  (1, 1964)\t1\n",
      "  :\t:\n",
      "  (825, 274)\t1\n",
      "  (825, 323)\t1\n",
      "  (825, 1829)\t1\n",
      "  (825, 1969)\t1\n",
      "  (826, 928)\t1\n",
      "  (826, 1829)\t1\n",
      "  (827, 323)\t1\n",
      "  (827, 449)\t1\n",
      "  (827, 1964)\t2\n",
      "  (827, 2033)\t1\n",
      "  (828, 2064)\t1\n",
      "  (829, 449)\t1\n",
      "  (829, 938)\t1\n",
      "  (829, 1964)\t2\n",
      "  (829, 2033)\t1\n",
      "  (829, 2122)\t1\n",
      "  (829, 2140)\t1\n",
      "  (829, 2487)\t1\n",
      "  (829, 2510)\t1\n",
      "  (830, 461)\t1\n",
      "  (831, 323)\t1\n",
      "  (831, 449)\t1\n",
      "  (831, 1964)\t1\n",
      "  (831, 2221)\t1\n",
      "  (832, 194)\t1\n"
     ]
    }
   ],
   "source": [
    "# creating the word count\n",
    "\n",
    "word_counts = bag_of_words.transform(documents)\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1964)\t0.1323802684885344\n",
      "  (0, 1944)\t0.4222847703750066\n",
      "  (0, 1923)\t0.2660125192587425\n",
      "  (0, 1431)\t0.3806664126941908\n",
      "  (0, 1396)\t0.2409306124340554\n",
      "  (0, 1327)\t0.16508710343184035\n",
      "  (0, 1310)\t0.39793959179012905\n",
      "  (0, 1170)\t0.39793959179012905\n",
      "  (0, 1073)\t0.35632123410931327\n",
      "  (0, 219)\t0.243933368024236\n",
      "  (1, 2689)\t0.13609411976303154\n",
      "  (1, 2671)\t0.2614763497862767\n",
      "  (1, 2603)\t0.2227801647688002\n",
      "  (1, 2588)\t0.18470529210827996\n",
      "  (1, 2246)\t0.2501265672755872\n",
      "  (1, 2113)\t0.2227801647688002\n",
      "  (1, 2045)\t0.21021636280120004\n",
      "  (1, 2001)\t0.2501265672755872\n",
      "  (1, 1964)\t0.08698382895855365\n",
      "  (1, 1833)\t0.2227801647688002\n",
      "  (1, 1722)\t0.10060722515467523\n",
      "  (1, 1697)\t0.2774729697823741\n",
      "  (1, 1344)\t0.1520907397591289\n",
      "  (1, 1234)\t0.09929201939125203\n",
      "  (1, 1126)\t0.24132299201455426\n",
      "  :\t:\n",
      "  (825, 1969)\t0.46636984727813474\n",
      "  (825, 1829)\t0.6617425768701939\n",
      "  (825, 323)\t0.4791666858634781\n",
      "  (825, 274)\t0.33910944349217925\n",
      "  (826, 1829)\t0.6695574161325341\n",
      "  (826, 928)\t0.7427603021849811\n",
      "  (827, 2033)\t0.5787241002576577\n",
      "  (827, 1964)\t0.43001451745865404\n",
      "  (827, 449)\t0.5289088844805495\n",
      "  (827, 323)\t0.44768440052486314\n",
      "  (828, 2064)\t1.0\n",
      "  (829, 2510)\t0.14538632167332996\n",
      "  (829, 2487)\t0.41854364146271705\n",
      "  (829, 2140)\t0.41854364146271705\n",
      "  (829, 2122)\t0.3944141430488243\n",
      "  (829, 2033)\t0.3531644930556275\n",
      "  (829, 1964)\t0.2624149555845914\n",
      "  (829, 938)\t0.41854364146271705\n",
      "  (829, 449)\t0.32276492023924325\n",
      "  (830, 461)\t1.0\n",
      "  (831, 2221)\t0.6651702943469645\n",
      "  (831, 1964)\t0.22127836478931834\n",
      "  (831, 449)\t0.5443355437024355\n",
      "  (831, 323)\t0.46074198924855075\n",
      "  (832, 194)\t1.0\n"
     ]
    }
   ],
   "source": [
    "#training the tfidf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "weights = tfidf.fit_transform(word_counts)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'>\n"
     ]
    }
   ],
   "source": [
    "#to improve signals, STOP_WORDS must be eliminated\n",
    "\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "print(type(STOP_WORDS))\n",
    "STOP_WORDS_python = STOP_WORDS.union({'python'})             #adding the word python to the set of unimportant words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'run', 'run', 'run']\n",
      "['skip', 'skip', 'skip', 'skip', 'skeptic']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatizating through spaCy\n",
    "print([word.lemma_ for word in nlp('run runs running ran')])\n",
    "print([word.lemma_ for word in nlp('skip skips skipped skipping skeptic')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['python'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '\\n\\n', '\\n\\n\\n', '\"', \"'s\", '(', ')', ',', '-', '-PRON-', '.', '1', '2', '3', ':', ';', '<', '=', 'a', 'also', 'an', 'and', 'as', 'at', 'ball', 'be', 'block', 'but', 'by', 'c', 'can', 'captivity', 'class', 'code', 'design', 'development', 'division', 'do', 'eat', 'example', 'expression', 'feature', 'find', 'for', 'from', 'ft', 'function', 'have', 'implementation', 'in', 'include', 'into', 'java', 'language', 'large', 'length', 'library', 'like', 'list', 'long', 'm', 'many', 'module', 'more', 'most', 'name', 'not', 'object', 'of', 'on', 'one', 'operator', 'or', 'other', 'program', 'programming', 'python', 'reference', 'release', 'reticulate', 'small', 'snake', 'some', 'standard', 'statement', 'such', 'support', 'syntax', 'than', 'that', 'the', 'this', 'time', 'to', 'type', 'use', 'version', 'which', 'with', 'write']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatiing using SciKit Learn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#defining function for that\n",
    "def lemmatizer(text):\n",
    "    return [word.lemma_ for word in nlp(text)]\n",
    "\n",
    "stop_words_str = \"\".join(STOP_WORDS)\n",
    "stop_words_lemma = set(word.lemma_ for word in nlp(stop_words_str))\n",
    "\n",
    "#instantiating the vectorizer\n",
    "tfidf_lemma = TfidfVectorizer(max_features = 100,\n",
    "                       stop_words = stop_words_lemma.union({'Python'}),\n",
    "                       tokenizer = lemmatizer)\n",
    "\n",
    "tfidf_lemma.fit(documents)\n",
    "print(tfidf_lemma.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'be', 'run', 'faster', 'than', 'homos', 'can', 'run', 'quickly']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer('Dogs are running faster than homos can run quickly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['23 ft',\n",
       " 'auliya et',\n",
       " 'ball pythons',\n",
       " 'et al',\n",
       " 'floating point',\n",
       " 'ft length',\n",
       " 'functional programming',\n",
       " 'isbn 978',\n",
       " 'list comprehensions',\n",
       " 'number incremented',\n",
       " 'object oriented',\n",
       " 'oriented programming',\n",
       " 'programming language',\n",
       " 'programming languages',\n",
       " 'reference implementation',\n",
       " 'reticulated pythons',\n",
       " 'scripting language',\n",
       " 'standard library',\n",
       " 'van rossum',\n",
       " 'year old']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BiGrams Model\n",
    "\n",
    "bigram_counter = CountVectorizer(max_features=20,         #for dispaying top 20 bigrams\n",
    "                               ngram_range = (2,2),\n",
    "                               stop_words = STOP_WORDS.union({'python'}))\n",
    "\n",
    "bigram_counter.fit(documents)\n",
    "bigram_counter.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
